{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD9AQHCQ9hEG"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EgpLQ1G9lJf"
   },
   "source": [
    "## Change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SElz4lytIwif",
    "outputId": "473c0c8b-6e94-4a2c-fdc3-81d99d213a97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hf.kong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/hf.kong/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import all the libraries needed\n",
    "import nltk\n",
    "import re\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9pC_kCLLI7wX"
   },
   "outputs": [],
   "source": [
    "#function for building our own corpus\n",
    "def corpus(aml_path, dl_path, corpus_tokens):\n",
    "    \n",
    "    #regular expression for capturing websites\n",
    "    website_pattern = r'(http|ftp|https+:\\/\\/)?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])?'\n",
    "    \n",
    "    #regular expression for capturing email address\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "    \n",
    "    #regular expression for capturing in-text citation\n",
    "    author = r\"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "    etal = r\"(?:et al\\.?)\"\n",
    "    additional = f\"(?:,? (?:(?:and |& )?{author}|{etal}))\"\n",
    "    year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "    page_num = \"(?:, p\\.? [0-9]+)?\"\n",
    "    year = fr\"(?:, *{year_num}{page_num}| *\\({year_num}{page_num}\\))\"\n",
    "    citation_pattern = fr'\\b(?!(?:Although|Also)\\b){author}{additional}*{year}'\n",
    "    \n",
    "    citation_pattern_2 = '\\(([^()\\d]*\\d[^()]*)'\n",
    "    \n",
    "    \n",
    "    with open(aml_path, 'r', encoding='utf8') as fileinput:\n",
    "        for line in fileinput:\n",
    "            line = line.lower()    #convert string into lowercases\n",
    "            output = re.findall(email_pattern, line)    #capture email address in the string\n",
    "            output2 = [\"\".join(x) for x in re.findall(website_pattern, line)]    #capture website in the string\n",
    "            output3 = re.findall(citation_pattern, line) #capture formal in-text citation\n",
    "            output4 = re.findall(citation_pattern_2, line) #capture improperly written in-text citation\n",
    "            \n",
    "            citation_output = list(set(output3 + output4)) #combine and remove duplication, if any\n",
    "            \n",
    "            words = nltk.word_tokenize(line)    #tokenize the string\n",
    "\n",
    "            for word in words:\n",
    "                if word.isalnum() and not word.isdigit() and len(word)>1 and word not in output and word not in output2 and word not in citation_output and word not in corpus_tokens:\n",
    "                    corpus_tokens.append(word)\n",
    "    fileinput.close()   \n",
    "\n",
    "    with open(dl_path, 'r', encoding='utf8') as fileinput:\n",
    "        for line in fileinput:\n",
    "            line = line.lower()\n",
    "            output = re.findall(email_pattern, line)\n",
    "            output2 = [\"\".join(x) for x in re.findall(website_pattern, line)]\n",
    "            output3 = re.findall(citation_pattern, line)\n",
    "            output4 = re.findall(citation_pattern_2, line)\n",
    "            \n",
    "            citation_output = list(set(output3 + output4))\n",
    "            \n",
    "            words = nltk.word_tokenize(line)\n",
    "\n",
    "            for word in words:\n",
    "                if word.isalnum() and not word.isdigit() and len(word)>1 and word not in output and word not in output2 and word not in citation_output and word not in corpus_tokens:\n",
    "                    corpus_tokens.append(word)\n",
    "    fileinput.close()\n",
    "    \n",
    "    return corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_detection(input_text):\n",
    "\n",
    "    error_list = []\n",
    "\n",
    "    #regular expression for capturing websites\n",
    "    website_pattern = r'(http|ftp|https+:\\/\\/)?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])?'\n",
    "    #regular expression for capturing email address\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "    #regular expression for capturing in-text citation\n",
    "    author = r\"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "    etal = r\"(?:et al\\.?)\"\n",
    "    additional = f\"(?:,? (?:(?:and |& )?{author}|{etal}))\"\n",
    "    year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "    page_num = \"(?:, p\\.? [0-9]+)?\"\n",
    "    year = fr\"(?:, *{year_num}{page_num}| *\\({year_num}{page_num}\\))\"\n",
    "    citation_pattern = fr'\\b(?!(?:Although|Also|Similarly)\\b){author}{additional}*{year}' \n",
    "    #regex for less formally written in-text citation\n",
    "    citation_pattern_2 = '\\(([^()\\d]*\\d[^()]*)'\n",
    "    #regular expression for abbreviation within parenthesis\n",
    "    abbrev_pattern = r'([A-Z]*\\s?[A-Z]+[^a-z0-9\\W])'\n",
    "\n",
    "    #perform regex operation on the entire input text, instead of on the word level\n",
    "    citation_output = re.findall(citation_pattern, input_text)#must process before lower() since the pattern is sensitive to it\n",
    "    citation_output = [x.lower() for x in citation_output]\n",
    "    abrv_output = re.findall(abbrev_pattern, input_text) #regex based on capital so need to put before lower\n",
    "    abrv_output = list(set([x.lower() for x in abrv_output])) #convert to lower for comparison \n",
    "    abrv_output = [j.replace(' ', '') for j in abrv_output] #remove space of the extracted abrv\n",
    "\n",
    "    input_text = input_text.lower()\n",
    "\n",
    "    email_output = re.findall(email_pattern, input_text)\n",
    "    website_output = [\"\".join(x) for x in re.findall(website_pattern, input_text)]\n",
    "    citation_output_2 = re.findall(citation_pattern_2, input_text)\n",
    "    citation_output_2 = [x for x in citation_output_2 if len(x)!=4] #remove year in () from in-text citation\n",
    "    \n",
    "    mark_list = ['(', ')', '[', ']', ',', '.', ':', ';'] \n",
    "\n",
    "    #combine all regex output, except abbrv\n",
    "    combined_output = list(set(email_output + website_output + citation_output + citation_output_2))\n",
    "\n",
    "    updated_output = []\n",
    "\n",
    "    #to remove in-text citation with last name of author only\n",
    "    combined_citation = list(set(citation_output+citation_output_2))\n",
    "    author_name = [k.split()[0] for k in combined_citation if ';' not in k]\n",
    "    author_name_2 = [j.split()[0] for j in nltk.flatten([k.split(';') for k in combined_citation if ';' in k])]\n",
    "    author_list = list(set(author_name+author_name_2))\n",
    "    author_list\n",
    "\n",
    "    #to make sure that the combined output is free from those marks and re-update to updated_output\n",
    "    for item in combined_output:\n",
    "        for mark in mark_list:\n",
    "            if mark in item:\n",
    "                item = item.replace(mark, '')\n",
    "            else:\n",
    "                continue\n",
    "        updated_output.append(item)\n",
    "\n",
    "    #to remove those regex output from the text, so that they're not involved during error detection\n",
    "    #compare to both combined and updated to reduce the chance of miss captured words\n",
    "    \n",
    "    combined_output.sort(key=len, reverse=True) #prioritize longer string first, if not the short string will replace it\n",
    "    updated_output.sort(key=len, reverse=True)\n",
    "\n",
    "    #input_text = input_text.replace(\"’s\", '') #different style of in-text citation\n",
    "    apostrophe_result = re.findall(\"\\w+\\’s|\\w+\\'s\", input_text)\n",
    "    for i in apostrophe_result:\n",
    "        if i in input_text:\n",
    "            input_text = input_text.replace(i, '')\n",
    "\n",
    "    for i in combined_output:\n",
    "        if i in input_text:\n",
    "            input_text = input_text.replace(i, '')\n",
    "\n",
    "    for i in updated_output:\n",
    "        if i in input_text:\n",
    "            input_text = input_text.replace(i, '')\n",
    "\n",
    "    for mark in mark_list:\n",
    "        input_text = input_text.replace(mark, '')\n",
    "\n",
    "    #check against the corpus\n",
    "    input_words = input_text.split()\n",
    "    for word in input_words:\n",
    "        if len(word)>1 and not word.isdigit() and word not in error_list and word not in abrv_output and word not in author_list:\n",
    "            if word not in corpus_tokens:\n",
    "                error_list.append(word)\n",
    "\n",
    "    #remove word with dash which actually existed in corpus from error list            \n",
    "    temp_list = []\n",
    "    dash_mark = '-'\n",
    "    for word in error_list:\n",
    "        if dash_mark in word:\n",
    "            split_word = word.split(dash_mark)\n",
    "            for i in range(len(split_word)):\n",
    "                if split_word[i] in corpus_tokens:\n",
    "                    temp_list.append(word)\n",
    "                    break\n",
    "     \n",
    "    for j in temp_list:\n",
    "        error_list.remove(j)\n",
    "    \n",
    "    #remove different lemma where the root is within the corpus\n",
    "    #for word in error_list:\n",
    "    #    if word in lexeme(word):\n",
    "    #        error_list.remove(word)\n",
    "    \n",
    "    return error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = %pwd\n",
    "folder = 'NLP assignment'\n",
    "aml_path = 'Corpus_PDFMiner_AMLtb.txt'\n",
    "dl_path = 'Corpus_PDFMiner_DLtb.txt'\n",
    "\n",
    "aml_path = os.path.join(current_dir, folder, aml_path)\n",
    "dl_path = os.path.join(current_dir, folder, dl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Generate corpus\n",
    "#aml_path = 'Corpus_PDFMiner_AMLtb.txt'\n",
    "#dl_path = 'Corpus_PDFMiner_DLtb.txt'\n",
    "\n",
    "#generate our own corpus\n",
    "corpus_tokens = []\n",
    "corpus_tokens = corpus(aml_path, dl_path, corpus_tokens)\n",
    "\n",
    "#built-in default in nltk\n",
    "wordlist = set(brown.words())\n",
    "\n",
    "#final corpus\n",
    "corpus_tokens += wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = %pwd\n",
    "folder = 'NLP assignment'\n",
    "path1 = 'input_text1.txt'\n",
    "path2 = 'input_text2.txt'\n",
    "path3 = 'input_text3.txt'\n",
    "\n",
    "input_text1 = os.path.join(current_dir, folder, path1)\n",
    "input_text2 = os.path.join(current_dir, folder, path2)\n",
    "input_text3 = os.path.join(current_dir, folder, path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "txt1 = Path(input_text1).read_text() #read txt file as save it as a string variable\n",
    "txt2 = Path(input_text2).read_text()\n",
    "txt3 = Path(input_text3).read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list_1 = error_detection(txt1)\n",
    "error_list_2 = error_detection(txt2)\n",
    "error_list_3 = error_detection(txt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import edit_distance as ED\n",
    "import heapq\n",
    "\n",
    "def word_suggestion(error_list, corpus, transOP=False, suggestion_no=5): #default to Levenshtein if not specify\n",
    "    \n",
    "    total_dict = {}\n",
    "    \n",
    "    for word1 in error_list:\n",
    "        ED_dict = {}\n",
    "        ED_dict1 = {}\n",
    "        sort_dict = {}\n",
    "        for word2 in corpus:\n",
    "            if abs(len(word1) - len(word2)) <= 2:\n",
    "                distance = ED(word1, word2, transpositions=transOP)\n",
    "                ED_dict[word2] = distance\n",
    "                if distance <= 1:\n",
    "                    ED_dict1[word2] = distance\n",
    "        \n",
    "        sort_dict['Top 5'] = heapq.nsmallest(suggestion_no, ED_dict, key=ED_dict.get)\n",
    "        sort_dict['D1'] = heapq.nsmallest(suggestion_no, ED_dict1, key=ED_dict1.get)\n",
    "        total_dict[word1] = sort_dict #use of nested dict to record error frequency if needed\n",
    "         \n",
    "    return total_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thst': ['test', 'that', 'thet', 'this', 'the']}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a = ['that', 'test', 'thet', 'this', 'the']\n",
    "list_b = ['test', 'that', 'thet']\n",
    "test_error = 'thst'\n",
    "test_dict = {}\n",
    "\n",
    "test_dict[test_error] = list_b\n",
    "for candidate in list_a:\n",
    "    if candidate not in list_b:\n",
    "        test_dict[test_error].append(candidate)\n",
    "\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_suggestion(suggestion_list):\n",
    "    for k, v in suggestion_list.items():\n",
    "        try:\n",
    "            print(f\"{k}: {v['Top 5']}\\n\")\n",
    "        except KeyError:\n",
    "            print(f\"{k}: {v}\\n\")\n",
    "        except TypeError:\n",
    "            print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Text 1 - Damerau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convaludional: ['convolutional', 'covolutional', 'conventional', 'convolution', 'convolutions']\n",
      "\n",
      "exemplary: ['exemplar', 'exemplify', 'example', 'examples', 'explain']\n",
      "\n",
      "multipel: ['multiple', 'multiply', 'multiplied', 'multiplies', 'multilabel']\n",
      "\n",
      "dato: ['data', 'date', 'to', 'at', 'day']\n",
      "\n",
      "activadion: ['activation', 'activations', 'motivation', 'activating', 'inactivation']\n",
      "\n",
      "regulalaisation: ['regularization', 'generalization', 'visualization', 'recalibration', 'serialization']\n",
      "\n",
      "architechtural: ['architectural', 'architecture', 'architectures', 'architecture10', 'architecture11']\n",
      "\n",
      "taxonomy: ['economy', 'Saxony', 'autonomy', 'Economy', 'topology']\n",
      "\n",
      "architechtures: ['architectures', 'architecture', 'architecture10', 'architecture11', 'architecture22']\n",
      "\n",
      "Time taken: 18.73s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "suggestions_1 = word_suggestion(error_list_1, corpus_tokens)\n",
    "display_suggestion(suggestions_1)\n",
    "print(f\"Time taken: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convaludional: ['convolutional', 'covolutional', 'conventional', 'convolution', 'convolutions']\n",
      "\n",
      "exemplary: ['exemplar', 'exemplify', 'example', 'examples', 'explain']\n",
      "\n",
      "multipel: ['multiple', 'multiply', 'Multiple', 'multiplied', 'multiplies']\n",
      "\n",
      "dato: ['data', 'date', 'to', 'at', 'day']\n",
      "\n",
      "activadion: ['activation', 'activations', 'motivation', 'activating', 'inactivation']\n",
      "\n",
      "regulalaisation: ['regularization', 'generalization', 'visualization', 'recalibration', 'serialization']\n",
      "\n",
      "architechtural: ['architectural', 'architecture', 'architectures', 'architecture10', 'architecture11']\n",
      "\n",
      "taxonomy: ['economy', 'Saxony', 'autonomy', 'Economy', 'topology']\n",
      "\n",
      "architechtures: ['architectures', 'architecture', 'architecture10', 'architecture11', 'architecture22']\n",
      "\n",
      "Time taken: 20.90s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dsuggestions_1 = word_suggestion(error_list_1, corpus_tokens, transOP=True)\n",
    "display_suggestion(dsuggestions_1)\n",
    "print(f\"Time taken: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Text 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thst: ['that', 'test', 'thet', 'this', 'the']\n",
      "\n",
      "modalling: ['modeling', 'mailing', 'meddling', 'falling', 'calling']\n",
      "\n",
      "algarithm: ['algorithm', 'algorithms', 'logarithm', 'algorithmic', 'algorithms9']\n",
      "\n",
      "semantics: ['semantic', 'romantics', 'pedantic', 'secants', 'seventies']\n",
      "\n",
      "customr: ['customer', 'custom', 'customs', 'customers', 'custody']\n",
      "\n",
      "modelling: ['modeling', 'yodeling', 'selling', 'meddling', 'compelling']\n",
      "\n",
      "segmentayion: ['segmentation', 'augmentation', 'regimentation', 'fermentation', 'sedimentation']\n",
      "\n",
      "bondari: ['boundary', 'bondage', 'poojari', 'binary', 'wonder']\n",
      "\n",
      "acuraxy: ['accuracy', 'accurate', 'array', 'crazy', 'curtly']\n",
      "\n",
      "segnet: ['senet', 'segment', 'sent', 'vggnet', 'resnet']\n",
      "\n",
      "Time taken: 26.84s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "suggestions_2 = word_suggestion(error_list_2, corpus_tokens)\n",
    "display_suggestion(suggestions_2)\n",
    "print(f\"Time taken: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thst: ['that', 'test', 'thet', 'this', 'the']\n",
      "\n",
      "modalling: ['modeling', 'mailing', 'meddling', 'falling', 'calling']\n",
      "\n",
      "algarithm: ['algorithm', 'algorithms', 'logarithm', 'algorithmic', 'algorithms9']\n",
      "\n",
      "semantics: ['semantic', 'romantics', 'pedantic', 'secants', 'seventies']\n",
      "\n",
      "customr: ['customer', 'custom', 'customs', 'customers', 'custody']\n",
      "\n",
      "modelling: ['modeling', 'yodeling', 'selling', 'meddling', 'compelling']\n",
      "\n",
      "segmentayion: ['segmentation', 'augmentation', 'regimentation', 'fermentation', 'sedimentation']\n",
      "\n",
      "bondari: ['boundary', 'bondage', 'poojari', 'binary', 'wonder']\n",
      "\n",
      "acuraxy: ['accuracy', 'accurate', 'array', 'crazy', 'curtly']\n",
      "\n",
      "segnet: ['senet', 'segment', 'sent', 'vggnet', 'resnet']\n",
      "\n",
      "Time taken: 27.79s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dsuggestions_2 = word_suggestion(error_list_2, corpus_tokens, transOP=True)\n",
    "display_suggestion(dsuggestions_2)\n",
    "print(f\"Time taken: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Text 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemplary: ['exemplar', 'exemplify', 'example', 'examples', 'explain']\n",
      "\n",
      "academa: ['academy', 'academic', 'Academy', 'academeh', 'camera']\n",
      "\n",
      "at&t: ['att', 'at', 'art', 'act', 'txt']\n",
      "\n",
      "frontruners: ['frontiers', 'containers', 'forerunners', 'intruders', 'consumers']\n",
      "\n",
      "visiin: ['vision', 'visit', 'viii', 'visits', 'visiting']\n",
      "\n",
      "cvcompetitions: ['competitions', 'computations', 'compositions', 'decomposition', 'compensations']\n",
      "\n",
      "multilayered: ['multilayer', 'multiplayer', 'multilateral', 'multilabel', 'mymultilayer']\n",
      "\n",
      "tranformasions: ['transformations', 'transformation', 'conformations', 'malformations', 'transforming']\n",
      "\n",
      "jetnels: ['kernels', 'jeunes', 'jewels', 'wetness', 'details']\n",
      "\n",
      "multi-tasking: ['multitasking', 'muddy-tasting', 'multiplying', 'multibackend', 'multimachine']\n",
      "\n",
      "topological: ['theological', 'biological', 'topologies', 'sociological', 'Geological']\n",
      "\n",
      "ventral: ['central', 'Central', 'general', 'neutral', 'mental']\n",
      "\n",
      "pathway: ['pathways', 'pathak', 'Hathaway', 'parkway', 'hatchway']\n",
      "\n",
      "Time taken: 29.43s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "suggestions_3 = word_suggestion(error_list_3, corpus_tokens)\n",
    "display_suggestion(suggestions_3)\n",
    "print(f\"Time taken: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemplary: ['exemplar', 'exemplify', 'example', 'examples', 'explain']\n",
      "\n",
      "academa: ['academy', 'academic', 'Academy', 'academeh', 'camera']\n",
      "\n",
      "at&t: ['att', 'at', 'art', 'act', 'txt']\n",
      "\n",
      "frontruners: ['frontiers', 'containers', 'forerunners', 'intruders', 'consumers']\n",
      "\n",
      "visiin: ['vision', 'visit', 'viii', 'visits', 'visiting']\n",
      "\n",
      "cvcompetitions: ['competitions', 'computations', 'compositions', 'decomposition', 'compensations']\n",
      "\n",
      "multilayered: ['multilayer', 'multiplayer', 'multilateral', 'multilabel', 'mymultilayer']\n",
      "\n",
      "tranformasions: ['transformations', 'transformation', 'conformations', 'malformations', 'transforming']\n",
      "\n",
      "jetnels: ['kernels', 'jeunes', 'jewels', 'wetness', 'details']\n",
      "\n",
      "multi-tasking: ['multitasking', 'muddy-tasting', 'multiplying', 'multibackend', 'multimachine']\n",
      "\n",
      "topological: ['theological', 'biological', 'topologies', 'sociological', 'Geological']\n",
      "\n",
      "ventral: ['central', 'Central', 'general', 'neutral', 'mental']\n",
      "\n",
      "pathway: ['pathways', 'pathak', 'Hathaway', 'parkway', 'hatchway']\n",
      "\n",
      "Time taken: 30.99s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dsuggestions_3 = word_suggestion(error_list_3, corpus_tokens, transOP=True)\n",
    "display_suggestion(dsuggestions_3)\n",
    "print(f\"Time taken: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_channel_model(suggestions, path='channel.xlsx'):\n",
    "\n",
    "    final_channel = {}\n",
    "    error_list = [k for k in suggestions.keys()]\n",
    "    potential_candidates = [v['D1'] for v in suggestions.values()]\n",
    "\n",
    "    for i in range(len(error_list)):\n",
    "        channel = {}\n",
    "        error = error_list[i]\n",
    "        candidates = potential_candidates[i]\n",
    "        if len(candidates)<2: continue\n",
    "        else:\n",
    "            for word in candidates:\n",
    "\n",
    "                if len(error) == len(word):\n",
    "\n",
    "                    dict1 = Counter(error)\n",
    "                    dict2 = Counter(word)\n",
    "\n",
    "                    if dict1 == dict2:\n",
    "                        #both words have same alphabets, transposition\n",
    "                        num = 0\n",
    "                        while num < len(error):\n",
    "                            #alphabets match\n",
    "                            if error[num] == word[num]:\n",
    "                                num += 1      \n",
    "                            else:\n",
    "                                edit = ''.join(reversed(word[num:num+2]))+'|'+word[num:num+2]\n",
    "                                edit = edit.replace(\" \", \"\")\n",
    "                                break\n",
    "\n",
    "                    else:\n",
    "                        #substitution\n",
    "                        num = 0\n",
    "                        while num < len(error):\n",
    "                            #alphabets match\n",
    "                            if error[num] == word[num]:\n",
    "                                num += 1      \n",
    "                            else:\n",
    "                                edit = error[num]+'|'+word[num]\n",
    "                                edit = edit.replace(\" \", \"\")\n",
    "                                break\n",
    "\n",
    "                elif len(error) < len(word):\n",
    "                    #deletion\n",
    "                    num = 0\n",
    "                    while num < len(error):\n",
    "                        #alphabets match\n",
    "                        if error[num] == word[num]:\n",
    "                            num += 1      \n",
    "                        else:\n",
    "                            edit = word[num-1]+'|'+word[num-1:num+1]\n",
    "                            edit = edit.replace(\" \", \"\")\n",
    "                            break\n",
    "\n",
    "                        #deletion at end of string\n",
    "                        if num == len(error):\n",
    "                            edit = word[num-1]+'|'+word[num-1:]\n",
    "                            edit = edit.replace(\" \", \"\")\n",
    "\n",
    "                else:\n",
    "                    #insertion\n",
    "                    num = 0\n",
    "                    while num < len(word):\n",
    "\n",
    "                        #alphabets match\n",
    "                        if error[num] == word[num]:\n",
    "                            num += 1      \n",
    "                        else:\n",
    "                            edit = '>'+error[num]+'|>'\n",
    "                            edit = edit.replace(\" \", \"\")\n",
    "                            break\n",
    "\n",
    "                        #insertion at the end of string\n",
    "                        if num == len(word):\n",
    "                            edit = '>'+error[num]+'|>'\n",
    "\n",
    "\n",
    "                channel[word] = edit\n",
    "        final_channel[error] =channel\n",
    "    \n",
    "    #computation of probability\n",
    "    df = pd.read_excel(path)\n",
    "    final_dict = {}\n",
    "    for k, v in final_channel.items():\n",
    "        prob_dict = {}\n",
    "        for k1, v1 in v.items():\n",
    "            prob = df.loc[df['edit'] == v1, 'prob'].tolist()\n",
    "            prob_dict[k1] = prob\n",
    "        final_dict[k] = heapq.nlargest(len(v.items()), prob_dict, key=prob_dict.get)\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dato': ['data', 'date']}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suggestions_1, suggestions_2, suggestions_3\n",
    "noisy_channel_model(suggestions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thst': ['test', 'that', 'thet'],\n",
       " 'customr': ['customer', 'customs', 'custom']}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel_model(suggestions_2) #noisy output different order for error 'thst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ventral': ['central', 'Central']}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel_model(suggestions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dsuggestions_1, dsuggestions_2, dsuggestions_3\n",
    "NC1 = noisy_channel_model(dsuggestions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC2 = noisy_channel_model(dsuggestions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC3 = noisy_channel_model(dsuggestions_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of candidates from MED and NCM to produce potential candidates that are ranked in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_candidates(MED_cand, NC_cand):\n",
    "\n",
    "    combined_cand = {}\n",
    "    for k, v in MED_cand.items():\n",
    "        if len(v['D1']) >= 2:\n",
    "            combined_cand[k] = v['D1']\n",
    "            for candidate in v['Top 5']:\n",
    "                if candidate not in v['D1']:\n",
    "                    combined_cand[k].append(candidate)\n",
    "        else:\n",
    "            combined_cand[k] = v['Top 5']\n",
    "            \n",
    "    return combined_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1_correction = combined_candidates(dsuggestions_2, NC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thst: ['that', 'test', 'thet', 'this', 'the']\n",
      "\n",
      "modalling: ['modeling', 'mailing', 'meddling', 'falling', 'calling']\n",
      "\n",
      "algarithm: ['algorithm', 'algorithms', 'logarithm', 'algorithmic', 'algorithms9']\n",
      "\n",
      "semantics: ['semantic', 'romantics', 'pedantic', 'secants', 'seventies']\n",
      "\n",
      "customr: ['customer', 'custom', 'customs', 'customers', 'custody']\n",
      "\n",
      "modelling: ['modeling', 'yodeling', 'selling', 'meddling', 'compelling']\n",
      "\n",
      "segmentayion: ['segmentation', 'augmentation', 'regimentation', 'fermentation', 'sedimentation']\n",
      "\n",
      "bondari: ['boundary', 'bondage', 'poojari', 'binary', 'wonder']\n",
      "\n",
      "acuraxy: ['accuracy', 'accurate', 'array', 'crazy', 'curtly']\n",
      "\n",
      "segnet: ['senet', 'segment', 'sent', 'vggnet', 'resnet']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_suggestion(txt1_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP assignment preprocessing (JoYen).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
